{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../../model/GPN_human/checkpoint-2000000 were not used when initializing ConvNetModel: ['cls.decoder.2.weight', 'cls.decoder.0.weight', 'cls.decoder.3.weight', 'cls.decoder.2.bias', 'cls.decoder.3.bias', 'cls.decoder.0.bias']\n",
      "- This IS expected if you are initializing ConvNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ConvNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import gpn.model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "model = AutoModel.from_pretrained(\"../../model/GPN_human/checkpoint-2000000\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../model/GPN_human/checkpoint-2000000\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenti-MPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459/2459 [05:55<00:00,  6.92it/s]\n",
      "100%|██████████| 308/308 [00:44<00:00,  6.92it/s]\n",
      "100%|██████████| 308/308 [00:46<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "celltype = 'K562'\n",
    "\n",
    "file = h5py.File('../../data/lenti_MPRA/'+celltype+'_data.h5','r')\n",
    "gpn_output = h5py.File('../../data/lenti_MPRA_embed/gpn_'+celltype+'.h5','a')\n",
    "batch_size = 128\n",
    "for dataset in ['seq_train','seq_valid','seq_test']:\n",
    "    gpn_output.create_dataset(name='x'+dataset[3:],shape=(len(file[dataset]),len(file[dataset][0]),512))\n",
    "    output_cache = []\n",
    "    for i in tqdm(range(0,len(file[dataset]),batch_size)):\n",
    "        seq = file[dataset][i:i+batch_size].astype('U230')\n",
    "        input_ids = tokenizer(seq.tolist(), return_tensors=\"pt\", return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        with torch.no_grad():\n",
    "            output_seq = model(input_ids.to('cuda')).last_hidden_state.cpu().detach().numpy()\n",
    "        gpn_output['x'+dataset[3:]][i:i+len(output_seq)] = output_seq\n",
    "    gpn_output.create_dataset(name='y'+dataset[3:],data = file['y'+dataset[3:]][:])\n",
    "gpn_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip/Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('../data/chip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    gpn_output = h5py.File('../data/chip/GPN/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 32\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "            seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\", return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "            with torch.no_grad():\n",
    "                output_seq = model(input_ids.to('cuda')).last_hidden_state.cpu().detach().numpy()\n",
    "            output_cache.extend(output_seq)\n",
    "        gpn_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        gpn_output.create_dataset(name='x_'+label,data = file['y_'+label][:],dtype='int') \n",
    "    gpn_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('../data/eclip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    gpn_output = h5py.File('../data/eclip/GPN/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 32\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['X_'+label]),batch_size)):\n",
    "            seq = file['X_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\", return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "            with torch.no_grad():\n",
    "                output_seq = model(input_ids.to('cuda')).last_hidden_state.cpu().detach().numpy()\n",
    "            output_cache.extend(output_seq)\n",
    "        gpn_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        gpn_output.create_dataset(name='y_'+label,data = file['Y_'+label][:],dtype='int') \n",
    "    gpn_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTSplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('../data/mtsplice/delta_logit.h5','r')\n",
    "gpn_output = h5py.File('../data/mtsplice/gpn_mt.h5','w')\n",
    "batch_size = 32\n",
    "for label in ('valid','test','train'):\n",
    "    l_cache = []\n",
    "    r_cache = [] \n",
    "    for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "        seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "        seq = utils.onehot_to_seq(seq)\n",
    "        clean_seq = [s if 'N' not in s else s.replace('N','[PAD]') for s in seq ]\n",
    "        input_ids = tokenizer(clean_seq, return_tensors=\"pt\", return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        l_input = input_ids[:,:400]\n",
    "        r_input = input_ids[:,400:]\n",
    "        with torch.no_grad():\n",
    "            l_output = model(l_input.to('cuda')).last_hidden_state.cpu().detach().numpy()\n",
    "            r_output = model(r_input.to('cuda')).last_hidden_state.cpu().detach().numpy()\n",
    "        l_cache.extend(l_output)\n",
    "        r_cache.extend(r_output)\n",
    "    gpn_output.create_dataset(name='xl_'+label,data = np.array(l_cache),dtype = 'float32')\n",
    "    gpn_output.create_dataset(name='xr_'+label,data = np.array(r_cache),dtype = 'float32')\n",
    "    gpn_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='float32') \n",
    "gpn_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('../data/rna_stable/insert_dataset.h5','r')\n",
    "gpn_output = h5py.File('../data/rna_stable/gpn_human_embed.h5','w')\n",
    "batch_size = 32\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    string_seq = utils.onehot_to_seq(onehot)\n",
    "\n",
    "    token_seq = tokenizer.batch_encode_plus(string_seq, max_length=512,padding = 'max_length')\n",
    "    output_cache = []\n",
    "    for seq_i in tqdm(range(0,len(token_seq['input_ids']),batch_size)):\n",
    "        seq_batch = torch.tensor(token_seq['input_ids'][seq_i:seq_i+batch_size]).to('cuda')\n",
    "        output_seq = model(seq_batch).last_hidden_state.cpu().detach().numpy()\n",
    "        output_cache.extend(output_seq[:,:173,:])\n",
    "    gpn_output.create_dataset(name=key,data = np.array(output_cache))\n",
    "    gpn_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])\n",
    "    gpn_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
