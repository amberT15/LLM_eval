{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztang/.conda/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/home/ztang/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel,BertConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py \n",
    "import os\n",
    "import glob\n",
    "import utils\n",
    "from torchinfo import summary\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True).to('cuda')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenti-MPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for representation extraction\n",
    "celltype = 'HepG2'\n",
    "file = h5py.File('../../data/lenti_MPRA/'+celltype+'_data.h5','r')\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "#Max size from BPE tokenizer\n",
    "batch_size = 128\n",
    "max_len = 0\n",
    "for dataset in ['seq_train','seq_valid','seq_test']:\n",
    "    for seq in file[dataset]:\n",
    "        seq = seq.decode()\n",
    "        input_ids = tokenizer([seq])[\"input_ids\"]\n",
    "        if len(input_ids[0]) > max_len:\n",
    "            max_len = len(input_ids[0])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output file location\n",
    "hidden_output = h5py.File('../../data/lenti_MPRA_embed/dnabert_'+celltype+'.h5','w')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for dataset in ['seq_train','seq_valid','seq_test']:\n",
    "    output_cache = []\n",
    "    for i in tqdm(range(0,len(file[dataset]),batch_size)):\n",
    "        seq = file[dataset][i:i+batch_size].astype('U230')\n",
    "        token = tokenizer(seq.tolist(), return_tensors=\"pt\",padding='max_length',max_length=59)\n",
    "        input_ids = token[\"input_ids\"].to('cuda')\n",
    "        attention_mask = token['attention_mask'].to('cuda')\n",
    "        hidden_states = model(input_ids,attention_mask)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        output_cache.extend(hidden_states)\n",
    "    hidden_output.create_dataset(name='x'+dataset[3:],data = np.array(output_cache))\n",
    "    hidden_output.create_dataset(name='y'+dataset[3:],data = file['y'+dataset[3:]][:])\n",
    "hidden_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Weight Intilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048895333\n",
      "0.122680694\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for n,v in model.named_parameters():\n",
    "    mean.append(np.mean(v.detach().cpu().numpy()))\n",
    "print(np.mean(mean))\n",
    "print(np.std(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weight in DNABERT model\n",
    "for m in model.modules():\n",
    "    m.apply(model._init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18116103\n",
      "0.38514945\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for n,v in model.named_parameters():\n",
    "    mean.append(np.mean(v.detach().cpu().numpy()))\n",
    "print(np.mean(mean))\n",
    "print(np.std(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['onehot_test', 'onehot_train', 'onehot_valid', 'seq_test', 'seq_train', 'seq_valid', 'y_test', 'y_train', 'y_valid']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset for representation extraction\n",
    "celltype = 'HepG2'\n",
    "file = h5py.File('../../data/lenti_MPRA/'+celltype+'_data.h5','r')\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1537/1537 [03:20<00:00,  7.68it/s]\n",
      "100%|██████████| 193/193 [00:24<00:00,  7.75it/s]\n",
      "100%|██████████| 193/193 [00:25<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#output file location\n",
    "hidden_output = h5py.File('../../data/lenti_MPRA_embed/randbert_'+celltype+'.h5','w')\n",
    "batch_size = 128\n",
    "\n",
    "for dataset in ['seq_train','seq_valid','seq_test']:\n",
    "    output_cache = []\n",
    "    for i in tqdm(range(0,len(file[dataset]),batch_size)):\n",
    "        seq = file[dataset][i:i+batch_size].astype('U230')\n",
    "        token = tokenizer(seq.tolist(), return_tensors=\"pt\",padding='max_length',max_length=59)\n",
    "        input_ids = token[\"input_ids\"].to('cuda')\n",
    "        attention_mask = token['attention_mask'].to('cuda')\n",
    "        hidden_states = model(input_ids,attention_mask)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        output_cache.extend(hidden_states)\n",
    "    hidden_output.create_dataset(name='x'+dataset[3:],data = np.array(output_cache))\n",
    "    hidden_output.create_dataset(name='y'+dataset[3:],data = file['y'+dataset[3:]][:])\n",
    "hidden_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip/Clip seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "file_list = glob.glob('../data/chip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    bert_output = h5py.File('../data/chip/DNABERT/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "            seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\",padding='max_length',max_length=57)[\"input_ids\"].to('cuda')\n",
    "            hidden_states = model(input_ids)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "            output_cache.extend(hidden_states)\n",
    "        bert_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        bert_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='int') \n",
    "    bert_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "file_list = glob.glob('../data/eclip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    bert_output = h5py.File('../data/eclip/DNABERT/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['X_'+label]),batch_size)):\n",
    "            seq = file['X_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\",padding='max_length',max_length=53)[\"input_ids\"].to('cuda')\n",
    "            hidden_states = model(input_ids)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "            output_cache.extend(hidden_states)\n",
    "        bert_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        bert_output.create_dataset(name='y_'+label,data = file['Y_'+label][:],dtype='int') \n",
    "    bert_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT Splice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('../data/alternative_splicing/delta_logit.h5','r')\n",
    "bert_output = h5py.File('../data/alternative_splicing/DNABERT_splice.h5','w')\n",
    "batch_size = 32\n",
    "max_len = 0\n",
    "for label in ('valid','test','train'):\n",
    "    l_cache = []\n",
    "    r_cache = [] \n",
    "    for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "        seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "        seq = utils.onehot_to_seq(seq)\n",
    "        clean_seq = seq\n",
    "        #clean_seq = [s if 'N' not in s else s.replace('N','[PAD]') for s in seq ]\n",
    "        l_seq = []\n",
    "        r_seq = []\n",
    "        for seq in clean_seq:\n",
    "            l_seq.append(seq[:400])\n",
    "            r_seq.append(seq[400:])\n",
    "        l_input = tokenizer(l_seq, return_tensors=\"pt\",padding='max_length',max_length=270)[\"input_ids\"].to('cuda')\n",
    "        r_input = tokenizer(r_seq, return_tensors=\"pt\",padding='max_length',max_length=270)[\"input_ids\"].to('cuda')\n",
    "\n",
    "        l_output = model(l_input)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        r_output = model(r_input)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        l_cache.extend(l_output)\n",
    "        r_cache.extend(r_output)\n",
    "    bert_output.create_dataset(name='xl_'+label,data = np.array(l_cache),dtype = 'float32')\n",
    "    bert_output.create_dataset(name='xr_'+label,data = np.array(r_cache),dtype = 'float32')\n",
    "    bert_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='float32') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bert_output.keys():\n",
    "    print(bert_output[key].shape)\n",
    "\n",
    "bert_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA-enlong data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = h5py.File('../data/RNAenlong/dnabert_embed.h5','w')\n",
    "file = h5py.File('../data/RNAenlong/insert_dataset.h5','r')\n",
    "batch_size = 32\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    string_seq = utils.onehot_to_seq(onehot)\n",
    "    token_seq = tokenizer(string_seq, return_tensors=\"pt\",padding='max_length',max_length=46)[\"input_ids\"].to('cuda')\n",
    "    output_cache = []\n",
    "    for seq_i in tqdm(range(0,len(token_seq),batch_size)):\n",
    "        hidden_states = model(token_seq[seq_i:seq_i+batch_size])[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        output_cache.extend(hidden_states)\n",
    "    bert_output.create_dataset(name=key,data = np.array(output_cache))\n",
    "    bert_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bert_output.keys():\n",
    "    print(bert_output[key].shape)\n",
    "\n",
    "bert_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
