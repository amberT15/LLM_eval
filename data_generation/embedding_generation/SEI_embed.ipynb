{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='2'\n",
    "import sys\n",
    "sys.path.append('../../model/sei_model/')\n",
    "import sei\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = torch.load('../../model/sei_model/sei.pth')\n",
    "clean_dict = {}\n",
    "for key in file_dict:\n",
    "    clean_key = key[13:]\n",
    "    clean_dict[clean_key] = file_dict[key]\n",
    "model = sei.Sei();\n",
    "model.load_state_dict(clean_dict)\n",
    "model.to('cuda').eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_extractor():\n",
    "    def __init__(self):\n",
    "        self.activation = {}\n",
    "    def get_activation(self,name):\n",
    "        def hook(model, input, output):\n",
    "            if name not in self.activation.keys():\n",
    "                self.activation[name] = []\n",
    "            self.activation[name].extend(output.detach().cpu().numpy())\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenti-MPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459/2459 [06:54<00:00,  5.93it/s]\n",
      "100%|██████████| 308/308 [00:52<00:00,  5.89it/s]\n",
      "100%|██████████| 308/308 [00:52<00:00,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "celltype = 'K562'\n",
    "embed_output = h5py.File('../../data/lenti_MPRA_embed/sei_'+celltype+'.h5','w')\n",
    "file = h5py.File('../../data/lenti_MPRA/'+celltype+'_data.h5','r')\n",
    "batch_size = 128\n",
    "pad_size = (4096-file['onehot_test'].shape[1])/2\n",
    "#LentiMPRA\n",
    "for dataset in ['onehot_train','onehot_valid','onehot_test']:\n",
    "    embed = embed_extractor()\n",
    "    model.spline_tr.register_forward_hook(embed.get_activation('s_out'))\n",
    "    for i in tqdm(range(0,len(file[dataset]),batch_size)):\n",
    "        seq = file[dataset][i:i+batch_size].transpose(0,2,1).astype('float32')\n",
    "        pad_seq = np.pad(seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "        with torch.no_grad():\n",
    "            output_seq = model(torch.from_numpy(pad_seq).to('cuda'))\n",
    "    embed_output.create_dataset(name='x'+dataset[6:],data = np.array(embed.activation['s_out']))\n",
    "    embed_output.create_dataset(name='y'+dataset[6:],data = file['y'+dataset[6:]][:])\n",
    "embed_output.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip-seq/Clip-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('../data/chip/*.h5')\n",
    "pad_size = (4096-200)/2\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    sei_output = h5py.File('../data/chip/sei/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        embed = embed_extractor()\n",
    "        model.spline_tr.register_forward_hook(embed.get_activation('s_out'))\n",
    "        for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "            seq = file['x_'+label][i:i+batch_size].astype('float32')\n",
    "            pad_seq = np.pad(seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "            with torch.no_grad():\n",
    "                output_seq = model(torch.from_numpy(pad_seq).to('cuda'))\n",
    "        #sanity check\n",
    "        assert len(embed.activation['s_out']) == file['y_'+label].shape[0]\n",
    "        sei_output.create_dataset(name='x_'+label,data = np.array(embed.activation['s_out']),dtype = 'float32')\n",
    "        sei_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='int') \n",
    "    sei_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('../data/eclip/*.h5')\n",
    "pad_size = (4096-200)/2\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    sei_output = h5py.File('../data/eclip/sei/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        embed = embed_extractor()\n",
    "        model.spline_tr.register_forward_hook(embed.get_activation('s_out'))\n",
    "        for i in tqdm(range(0,len(file['X_'+label]),batch_size)):\n",
    "            seq = file['X_'+label][i:i+batch_size][:,:4,:].astype('float32')\n",
    "            pad_seq = np.pad(seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "            with torch.no_grad():\n",
    "                output_seq = model(torch.from_numpy(pad_seq).to('cuda'))\n",
    "        #sanity check\n",
    "        assert len(embed.activation['s_out']) == file['Y_'+label].shape[0]\n",
    "        sei_output.create_dataset(name='x_'+label,data = np.array(embed.activation['s_out']),dtype = 'float32')\n",
    "        sei_output.create_dataset(name='y_'+label,data = file['Y_'+label][:],dtype='int')\n",
    "        embed.activation['s_out'] = []\n",
    "    sei_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT Splice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('../data/alternative_splicing/delta_logit.h5','r')\n",
    "sei_output = h5py.File('../data/alternative_splicing/sei_splice.h5','w')\n",
    "batch_size = 32\n",
    "pad_size = (4096-400)/2\n",
    "for label in ('valid','test','train'):\n",
    "    embed = embed_extractor()\n",
    "    model.spline_tr.register_forward_hook(embed.get_activation('s_out'))\n",
    "    l_cache = []\n",
    "    r_cache = []  \n",
    "    for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "        l_seq = []\n",
    "        r_seq = []\n",
    "        seq = file['x_'+label][i:i+batch_size].astype('float32')\n",
    "        seq = np.swapaxes(seq,1,2)\n",
    "        for s in seq:\n",
    "            l_seq.append(s[:,:400])\n",
    "            r_seq.append(s[:,400:])\n",
    "        l_pad = np.pad(l_seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "        r_pad = np.pad(r_seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "        with torch.no_grad():\n",
    "            output_seq = model(torch.from_numpy(l_pad).to('cuda'))\n",
    "            l_cache.extend(embed.activation['s_out'])\n",
    "            embed.activation={}\n",
    "            output_seq = model(torch.from_numpy(r_pad).to('cuda'))\n",
    "            r_cache.extend(embed.activation['s_out'])\n",
    "            embed.activation={}\n",
    "        \n",
    "    sei_output.create_dataset(name='xl_'+label,data = np.array(l_cache),dtype = 'float32')\n",
    "    sei_output.create_dataset(name='xr_'+label,data = np.array(r_cache),dtype = 'float32')\n",
    "    sei_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='float32') \n",
    "    sei_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 36/36 [00:01<00:00, 22.96it/s]\n",
      "100%|██████████| 286/286 [00:12<00:00, 23.75it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 23.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sei_output = h5py.File('../data/RNAenlong/sei_embed.h5','w')\n",
    "file = h5py.File('../data/RNAenlong/insert_dataset.h5','r')\n",
    "batch_size = 32\n",
    "pad_size = (4096-file['X_train'].shape[1])/2\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    embed = embed_extractor()\n",
    "    model.spline_tr.register_forward_hook(embed.get_activation('s_out'))\n",
    "    for i in tqdm(range(0,len(onehot),batch_size)):\n",
    "        seq = onehot[i:i+batch_size].astype('float32')\n",
    "        seq = np.swapaxes(seq,1,2)\n",
    "        pad_seq = np.pad(seq,((0,0),(0,0),(math.floor(pad_size),math.ceil(pad_size))))\n",
    "        with torch.no_grad():\n",
    "            output_seq = model(torch.from_numpy(pad_seq).to('cuda'))\n",
    "    sei_output.create_dataset(name=key,data = np.array(embed.activation['s_out']))\n",
    "    sei_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])\n",
    "sei_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
