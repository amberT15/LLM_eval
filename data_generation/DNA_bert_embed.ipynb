{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztang/.conda/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/home/ztang/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py \n",
    "import os\n",
    "import glob\n",
    "import utils\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True).to('cuda')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenti-MPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype = 'K562'\n",
    "file = h5py.File('/home/ztang/multitask_RNA/data/lenti_MPRA/'+celltype+'_data.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = h5py.File('../data/lenti_MPRA_embed/dnabert2_'+celltype+'.h5','w')\n",
    "batch_size = 128\n",
    "output_cache = []\n",
    "for i in tqdm(range(0,len(file['seq']),batch_size)):\n",
    "    seq = file['seq'][i:i+batch_size].astype('U230')\n",
    "    input_ids = tokenizer(seq.tolist(), return_tensors=\"pt\",padding='max_length',max_length=58)[\"input_ids\"].to('cuda')\n",
    "    hidden_states = model(input_ids)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "    output_cache.extend(hidden_states)\n",
    "hidden_output.create_dataset(name='seq',data = np.array(output_cache))\n",
    "hidden_output.create_dataset(name='mean',data = file['mean'][:])\n",
    "hidden_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip/Clip seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [00:32<00:00,  7.60it/s]\n",
      "100%|██████████| 36/36 [00:04<00:00,  7.91it/s]\n",
      "100%|██████████| 71/71 [00:09<00:00,  7.85it/s]\n",
      "100%|██████████| 212/212 [00:28<00:00,  7.49it/s]\n",
      "100%|██████████| 31/31 [00:03<00:00,  7.98it/s]\n",
      "100%|██████████| 61/61 [00:07<00:00,  7.80it/s]\n",
      "100%|██████████| 266/266 [00:35<00:00,  7.52it/s]\n",
      "100%|██████████| 38/38 [00:04<00:00,  7.83it/s]\n",
      "100%|██████████| 76/76 [00:09<00:00,  7.68it/s]\n",
      "100%|██████████| 478/478 [01:04<00:00,  7.37it/s]\n",
      "100%|██████████| 69/69 [00:08<00:00,  7.79it/s]\n",
      "100%|██████████| 137/137 [00:18<00:00,  7.60it/s]\n",
      "100%|██████████| 80/80 [00:10<00:00,  7.70it/s]\n",
      "100%|██████████| 12/12 [00:01<00:00,  7.87it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00,  7.81it/s]\n",
      "100%|██████████| 87/87 [00:11<00:00,  7.75it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  8.22it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.86it/s]\n",
      "100%|██████████| 200/200 [00:26<00:00,  7.44it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.89it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.87it/s]\n",
      "100%|██████████| 71/71 [00:09<00:00,  7.74it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.51it/s]\n",
      "100%|██████████| 21/21 [00:02<00:00,  8.05it/s]\n",
      "100%|██████████| 215/215 [00:28<00:00,  7.42it/s]\n",
      "100%|██████████| 31/31 [00:04<00:00,  7.70it/s]\n",
      "100%|██████████| 62/62 [00:07<00:00,  7.83it/s]\n",
      "100%|██████████| 198/198 [00:26<00:00,  7.45it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.93it/s]\n",
      "100%|██████████| 57/57 [00:07<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "file_list = glob.glob('../data/chip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    bert_output = h5py.File('../data/chip/DNABERT/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "            seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\",padding='max_length',max_length=57)[\"input_ids\"].to('cuda')\n",
    "            hidden_states = model(input_ids)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "            output_cache.extend(hidden_states)\n",
    "        bert_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        bert_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='int') \n",
    "    bert_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  8.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.37it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  8.38it/s]\n",
      "100%|██████████| 89/89 [00:11<00:00,  7.72it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  8.35it/s]\n",
      "100%|██████████| 26/26 [00:03<00:00,  8.42it/s]\n",
      "100%|██████████| 80/80 [00:10<00:00,  7.81it/s]\n",
      "100%|██████████| 12/12 [00:01<00:00,  8.78it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00,  8.42it/s]\n",
      "100%|██████████| 38/38 [00:04<00:00,  8.27it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  9.39it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.51it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  8.18it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00,  9.39it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  8.70it/s]\n",
      "100%|██████████| 179/179 [00:25<00:00,  6.92it/s]\n",
      "100%|██████████| 26/26 [00:03<00:00,  8.37it/s]\n",
      "100%|██████████| 52/52 [00:06<00:00,  8.11it/s]\n",
      "100%|██████████| 33/33 [00:04<00:00,  8.16it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.87it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.77it/s]\n",
      "100%|██████████| 67/67 [00:08<00:00,  7.89it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.72it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.28it/s]\n",
      "100%|██████████| 99/99 [00:13<00:00,  7.55it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.33it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.49it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  9.53it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  9.46it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "file_list = glob.glob('../data/eclip/*.h5')\n",
    "for file in file_list:\n",
    "    tf_name = file.split('/')[-1][:-7]\n",
    "    bert_output = h5py.File('../data/eclip/DNABERT/'+tf_name+'_200.h5','w')\n",
    "    batch_size = 128\n",
    "    file = h5py.File(file,'r')\n",
    "    for label in ('train','valid','test'):\n",
    "        output_cache = []  \n",
    "        for i in tqdm(range(0,len(file['X_'+label]),batch_size)):\n",
    "            seq = file['X_'+label][i:i+batch_size].astype('int')\n",
    "            seq = np.transpose(seq,(0,2,1))\n",
    "            seq = utils.onehot_to_seq(seq)\n",
    "            input_ids = tokenizer(seq, return_tensors=\"pt\",padding='max_length',max_length=53)[\"input_ids\"].to('cuda')\n",
    "            hidden_states = model(input_ids)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "            output_cache.extend(hidden_states)\n",
    "        bert_output.create_dataset(name='x_'+label,data = np.array(output_cache),dtype = 'float32')\n",
    "        bert_output.create_dataset(name='y_'+label,data = file['Y_'+label][:],dtype='int') \n",
    "    bert_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT Splice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:22<00:00,  1.48it/s]\n",
      "100%|██████████| 370/370 [04:05<00:00,  1.51it/s]\n",
      "100%|██████████| 1189/1189 [13:03<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "file = h5py.File('../data/alternative_splicing/delta_logit.h5','r')\n",
    "bert_output = h5py.File('../data/alternative_splicing/DNABERT_splice.h5','w')\n",
    "batch_size = 32\n",
    "max_len = 0\n",
    "for label in ('valid','test','train'):\n",
    "    l_cache = []\n",
    "    r_cache = [] \n",
    "    for i in tqdm(range(0,len(file['x_'+label]),batch_size)):\n",
    "        seq = file['x_'+label][i:i+batch_size].astype('int')\n",
    "        seq = utils.onehot_to_seq(seq)\n",
    "        clean_seq = seq\n",
    "        #clean_seq = [s if 'N' not in s else s.replace('N','[PAD]') for s in seq ]\n",
    "        l_seq = []\n",
    "        r_seq = []\n",
    "        for seq in clean_seq:\n",
    "            l_seq.append(seq[:400])\n",
    "            r_seq.append(seq[400:])\n",
    "        l_input = tokenizer(l_seq, return_tensors=\"pt\",padding='max_length',max_length=270)[\"input_ids\"].to('cuda')\n",
    "        r_input = tokenizer(r_seq, return_tensors=\"pt\",padding='max_length',max_length=270)[\"input_ids\"].to('cuda')\n",
    "\n",
    "        l_output = model(l_input)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        r_output = model(r_input)[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        l_cache.extend(l_output)\n",
    "        r_cache.extend(r_output)\n",
    "    bert_output.create_dataset(name='xl_'+label,data = np.array(l_cache),dtype = 'float32')\n",
    "    bert_output.create_dataset(name='xr_'+label,data = np.array(r_cache),dtype = 'float32')\n",
    "    bert_output.create_dataset(name='y_'+label,data = file['y_'+label][:],dtype='float32') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11840, 270, 768)\n",
      "(38028, 270, 768)\n",
      "(1088, 270, 768)\n",
      "(11840, 270, 768)\n",
      "(38028, 270, 768)\n",
      "(1088, 270, 768)\n",
      "(11840, 56, 2)\n",
      "(38028, 56, 2)\n",
      "(1088, 56, 2)\n"
     ]
    }
   ],
   "source": [
    "for key in bert_output.keys():\n",
    "    print(bert_output[key].shape)\n",
    "\n",
    "bert_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA-enlong data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:01<00:00, 32.02it/s]\n",
      "100%|██████████| 286/286 [00:08<00:00, 33.84it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 34.37it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_output = h5py.File('../data/RNAenlong/dnabert_embed.h5','w')\n",
    "file = h5py.File('../data/RNAenlong/insert_dataset.h5','r')\n",
    "batch_size = 32\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    string_seq = utils.onehot_to_seq(onehot)\n",
    "    token_seq = tokenizer(string_seq, return_tensors=\"pt\",padding='max_length',max_length=46)[\"input_ids\"].to('cuda')\n",
    "    output_cache = []\n",
    "    for seq_i in tqdm(range(0,len(token_seq),batch_size)):\n",
    "        hidden_states = model(token_seq[seq_i:seq_i+batch_size])[0].cpu().detach().numpy() # [1, sequence_length, 768]\n",
    "        output_cache.extend(hidden_states)\n",
    "    bert_output.create_dataset(name=key,data = np.array(output_cache))\n",
    "    bert_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 46, 768)\n",
      "(9131, 46, 768)\n",
      "(1149, 46, 768)\n",
      "(1137, 3)\n",
      "(9131, 3)\n",
      "(1149, 3)\n"
     ]
    }
   ],
   "source": [
    "for key in bert_output.keys():\n",
    "    print(bert_output[key].shape)\n",
    "\n",
    "bert_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
