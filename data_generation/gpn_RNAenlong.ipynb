{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at /home/ztang/multitask_RNA/model/GPN_human/checkpoint-2000000 were not used when initializing ConvNetModel: ['cls.decoder.2.weight', 'cls.decoder.3.bias', 'cls.decoder.0.weight', 'cls.decoder.0.bias', 'cls.decoder.2.bias', 'cls.decoder.3.weight']\n",
      "- This IS expected if you are initializing ConvNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ConvNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "from Bio import SeqIO\n",
    "import bioframe\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import h5py\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "from transformers import AutoModel,AutoTokenizer,AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gpn.model\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "\n",
    "#model = AutoModel.from_pretrained(\"/home/ztang/multitask_RNA/model/GPN_finetune/checkpoint-2000000\").to('cuda')\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"/home/ztang/multitask_RNA/model/GPN_finetune/checkpoint-2000000\")\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"gonzalobenegas/gpn-arabidopsis\").to('cuda')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gonzalobenegas/gpn-arabidopsis\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/home/ztang/multitask_RNA/model/GPN_human/checkpoint-2000000\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ztang/multitask_RNA/model/GPN_human/checkpoint-2000000\")\n",
    "\n",
    "model.eval();\n",
    "file = h5py.File('../../data/rna_stable/insert_dataset.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/home/ztang/.conda/envs/gpn_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 36/36 [00:02<00:00, 15.33it/s]\n",
      "100%|██████████| 286/286 [00:18<00:00, 15.73it/s]\n",
      "100%|██████████| 36/36 [00:02<00:00, 15.69it/s]\n"
     ]
    }
   ],
   "source": [
    "gpn_output = h5py.File('../../data/rna_stable/gpn_human_embed.h5','w')\n",
    "batch_size = 32\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    string_seq = utils.onehot_to_seq(onehot)\n",
    "\n",
    "    token_seq = tokenizer.batch_encode_plus(string_seq, max_length=512,padding = 'max_length')\n",
    "    output_cache = []\n",
    "    for seq_i in tqdm(range(0,len(token_seq['input_ids']),batch_size)):\n",
    "        seq_batch = torch.tensor(token_seq['input_ids'][seq_i:seq_i+batch_size]).to('cuda')\n",
    "        output_seq = model(seq_batch).last_hidden_state.cpu().detach().numpy()\n",
    "        output_cache.extend(output_seq[:,:173,:])\n",
    "    gpn_output.create_dataset(name=key,data = np.array(output_cache))\n",
    "    gpn_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpn_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149, 173, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(output_cache).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "citra_f = h5py.File('/home/ztang/multitask_RNA/data/rna_stable/citra_copy/gpn_human_embed.h5','r')\n",
    "local_f = h5py.File('/home/ztang/multitask_RNA/data/rna_stable/gpn_human_embed.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8284e1417b754e460c2bde3a4a4837c482fa82ceb7d52f4acbe340dd4b4559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
