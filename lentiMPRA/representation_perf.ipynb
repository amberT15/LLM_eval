{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import mpra_model\n",
    "import h5py\n",
    "importlib.reload(mpra_model)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import Ridge\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ridge/MLP model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######Starting with hyena embeddings for HepG2########\n",
      "MLP for mean embedding training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:01:00.427457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:85:00.0, compute capability: 8.0\n",
      "/home/ztang/.conda/envs/jax_tf/lib/python3.9/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2024-05-28 15:01:01.582622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-05-28 15:01:01.602813: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f52e452ef20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-28 15:01:01.602849: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-05-28 15:01:01.606237: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-28 15:01:01.622001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800\n",
      "2024-05-28 15:01:01.726220: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 1s 594us/step\n",
      "769/769 [==============================] - 1s 584us/step\n",
      "769/769 [==============================] - 1s 596us/step\n",
      "#######Starting with hyena embeddings for K562########\n",
      "MLP for mean embedding training...\n",
      "1230/1230 [==============================] - 1s 593us/step\n",
      "1230/1230 [==============================] - 1s 587us/step\n",
      "1230/1230 [==============================] - 1s 589us/step\n",
      "#######Starting with gpn embeddings for HepG2########\n",
      "MLP for mean embedding training...\n",
      "769/769 [==============================] - 1s 601us/step\n",
      "769/769 [==============================] - 1s 596us/step\n",
      "769/769 [==============================] - 1s 596us/step\n",
      "#######Starting with gpn embeddings for K562########\n",
      "MLP for mean embedding training...\n",
      "1230/1230 [==============================] - 1s 599us/step\n",
      "1230/1230 [==============================] - 1s 606us/step\n",
      "1230/1230 [==============================] - 1s 582us/step\n",
      "#######Starting with dnabert embeddings for HepG2########\n",
      "MLP for mean embedding training...\n",
      "769/769 [==============================] - 1s 602us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 612us/step\n",
      "769/769 [==============================] - 1s 593us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 613us/step\n",
      "769/769 [==============================] - 1s 599us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 615us/step\n",
      "#######Starting with dnabert embeddings for K562########\n",
      "MLP for mean embedding training...\n",
      "1230/1230 [==============================] - 1s 616us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 597us/step\n",
      "1230/1230 [==============================] - 1s 592us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 603us/step\n",
      "1230/1230 [==============================] - 1s 592us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 624us/step\n",
      "#######Starting with randbert embeddings for HepG2########\n",
      "MLP for mean embedding training...\n",
      "769/769 [==============================] - 1s 605us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 614us/step\n",
      "769/769 [==============================] - 1s 623us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 633us/step\n",
      "769/769 [==============================] - 1s 623us/step\n",
      "MLP for cls training...\n",
      "769/769 [==============================] - 1s 607us/step\n",
      "#######Starting with randbert embeddings for K562########\n",
      "MLP for mean embedding training...\n",
      "1230/1230 [==============================] - 1s 600us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 601us/step\n",
      "1230/1230 [==============================] - 1s 613us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 603us/step\n",
      "1230/1230 [==============================] - 1s 602us/step\n",
      "MLP for cls training...\n",
      "1230/1230 [==============================] - 1s 605us/step\n"
     ]
    }
   ],
   "source": [
    "ct_list = ['HepG2','K562']\n",
    "model_list = []\n",
    "LLM_list = []\n",
    "perf_list = []\n",
    "celltype_list = []\n",
    "for model_name in ['hyena','gpn','dnabert','randbert']:\n",
    "    for ct in ct_list:\n",
    "        print('#######Starting with ' + model_name + ' embeddings for ' + ct +'########')\n",
    "        f = h5py.File('../data/lenti_MPRA_embed/'+model_name+'_'+ct+'.h5', 'r')\n",
    "\n",
    "        x_train = f['x_train']\n",
    "        x_valid = f['x_valid']\n",
    "        x_test = f['x_test']\n",
    "        y_train = f['y_train']\n",
    "        y_valid = f['y_valid']\n",
    "        y_test = f['y_test']\n",
    "\n",
    "        #CLS token embedding doens't represent summary representation for conv based methods\n",
    "        if model_name == 'gpn' or model_name == 'hyena':\n",
    "            mean_train = np.mean(x_train,axis=1)\n",
    "            mean_valid = np.mean(x_valid,axis=1)\n",
    "            mean_test = np.mean(x_test,axis=1)\n",
    "            cls_train = None\n",
    "        else:\n",
    "            mean_train = np.mean(x_train[:,1:,:],axis=1)\n",
    "            mean_valid = np.mean(x_valid[:,1:,:],axis=1)\n",
    "            mean_test = np.mean(x_test[:,1:,:],axis=1)\n",
    "            cls_train = np.squeeze(x_train[:,:1,:])\n",
    "            cls_valid = np.squeeze(x_valid[:,:1,:])\n",
    "            cls_test = np.squeeze(x_test[:,:1,:])\n",
    "        \n",
    "        # Ridge regression\n",
    "        print('Ridge regression for CLS and Mean Embed')\n",
    "        embed_model = Ridge(0.001).fit(mean_train, y_train)\n",
    "\n",
    "        LLM_list.append(model_name)\n",
    "        model_list.append('Mean-embed-Ridge')\n",
    "        perf_list.append(scipy.stats.pearsonr(embed_model.predict(mean_test)[:,0],y_test[:,0])[0])\n",
    "        celltype_list.append(ct)\n",
    "\n",
    "        if cls_train is not None:\n",
    "            embed_model = Ridge(0.001).fit(cls_train, y_train)\n",
    "\n",
    "            LLM_list.append(model_name)\n",
    "            model_list.append('CLS-Ridge')\n",
    "            perf_list.append(scipy.stats.pearsonr(embed_model.predict(cls_test)[:,0],y_test[:,0])[0])\n",
    "            celltype_list.append(ct)\n",
    "\n",
    "        ## MLP model\n",
    "        for factor in [0.5,1,2]:\n",
    "            print('MLP for mean embedding training...')\n",
    "            model = mpra_model.rep_mlp(mean_train.shape[1],factor=factor)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "            earlyStopping_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=1e-8)\n",
    "            model.compile(optimizer=optimizer,\n",
    "                            loss='mean_squared_error',\n",
    "                            metrics=['mse'])\n",
    "            model.fit(\n",
    "                    mean_train,y_train[:,0],\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(mean_valid,y_valid[:,0]),\n",
    "                    callbacks=[earlyStopping_callback,reduce_lr],\n",
    "                    verbose=0,)\n",
    "            y_pred = model.predict(mean_test)\n",
    "\n",
    "            perf_list.append(scipy.stats.pearsonr(np.squeeze(y_pred),y_test[:,0])[0])\n",
    "            LLM_list.append(model_name)\n",
    "            celltype_list.append(ct)\n",
    "            model_list.append('Mean-embed-MLP%1.1f'%factor)\n",
    "            if cls_train is not None:\n",
    "                print('MLP for cls training...')\n",
    "                model = mpra_model.rep_mlp(cls_train.shape[1],factor=factor)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "                earlyStopping_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "                reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=1e-8)\n",
    "                model.compile(optimizer=optimizer,\n",
    "                                loss='mean_squared_error',\n",
    "                                metrics=['mse'])\n",
    "                model.fit(\n",
    "                        cls_train,y_train[:,0],\n",
    "                        epochs=100,\n",
    "                        batch_size=512,\n",
    "                        shuffle=True,\n",
    "                        validation_data = (cls_valid,y_valid[:,0]),\n",
    "                        callbacks=[earlyStopping_callback,reduce_lr],\n",
    "                        verbose=0,)\n",
    "                y_pred = model.predict(cls_test)\n",
    "\n",
    "                perf_list.append(scipy.stats.pearsonr(np.squeeze(y_pred),y_test[:,0])[0])\n",
    "                LLM_list.append(model_name)\n",
    "                celltype_list.append(ct)\n",
    "                model_list.append('CLS-MLP%1.1f'%factor)\n",
    "\n",
    "            del(model)\n",
    "            tf.keras.backend.clear_session()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame({'LLM':LLM_list,'Model':model_list,'Performance':perf_list,'Cell Type':celltype_list})\n",
    "perf_df.to_csv('./results/LLM_regression.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN structure hyperparameters\n",
    "cnn_config = {\n",
    "    'activation':'exponential',\n",
    "    'reduce_dim': 128,\n",
    "    'conv1_filter':196,\n",
    "    'conv1_kernel':7,\n",
    "    'dropout1':0.2,\n",
    "    'res_filter':5,\n",
    "    'res_layers':3,\n",
    "    'res_pool':5,\n",
    "    'res_dropout':0.2,\n",
    "    'conv2_filter':256,\n",
    "    'conv2_kernel':7,\n",
    "    'pool2_size':4,\n",
    "    'dropout2':0.2,\n",
    "    'dense':512,\n",
    "    'dense2':256,\n",
    "    'l_rate':0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_downstream_training(data_file,cnn_config,model_save_path,factor=1):\n",
    "    #Load dataset\n",
    "    f = h5py.File(data_file,'r')\n",
    "    x_train = f['x_train']\n",
    "    x_valid = f['x_valid']\n",
    "    x_test = f['x_test']\n",
    "    y_train = f['y_train']\n",
    "    y_valid = f['y_valid']\n",
    "    y_test = f['y_test']\n",
    "    #Construct model and training choices\n",
    "    model = mpra_model.rep_cnn(x_train[0].shape,cnn_config,factor = factor)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=cnn_config['l_rate'])\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['mse'])\n",
    "    earlyStopping_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.2,\n",
    "                patience=5, min_lr=1e-8)\n",
    "    #save trained model\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                    model_save_path,\n",
    "                                    monitor='val_loss',\n",
    "                                    save_best_only=True,\n",
    "                                    mode = 'min',\n",
    "                                    save_freq='epoch',)\n",
    "    model.fit(\n",
    "        x_train,y_train,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        validation_data = (x_valid,y_valid),\n",
    "        callbacks=[earlyStopping_callback,reduce_lr,checkpoint])\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    del model\n",
    "    del x_train,y_train,x_valid,y_valid\n",
    "\n",
    "    return scipy.stats.pearsonr(np.squeeze(y_pred),np.squeeze(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for gpn on HepG2 data\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 16:16:21.946214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:85:00.0, compute capability: 8.0\n",
      "/home/ztang/.conda/envs/jax_tf/lib/python3.9/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2024-05-28 16:16:23.834890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800\n",
      "2024-05-28 16:16:24.150785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-05-28 16:16:24.160100: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x55f4324f84e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-28 16:16:24.160132: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-05-28 16:16:24.238514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-28 16:16:24.323922: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "769/769 [==============================] - 8s 10ms/step\n",
      "Training for gpn on HepG2 data\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "769/769 [==============================] - 5s 6ms/step\n",
      "Training for gpn on HepG2 data\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    }
   ],
   "source": [
    "llm = []\n",
    "ds_model = []\n",
    "perf = []\n",
    "c_t = []\n",
    "\n",
    "for model in ['gpn','dnabert','hyena','randbert','sei']:\n",
    "    for ct in ['HepG2','K562']:\n",
    "        for factor in [0.5,1,2]:\n",
    "            print('Training for %s on %s data'%(model,ct))\n",
    "            data_file = ('../data/lenti_MPRA_embed/%s_%s.h5'%(model,ct))\n",
    "            model_save = ('../model/lenti_MPRA/%s%1.1f_%s.h5'%(model,factor,ct))\n",
    "            pr_perf = CNN_downstream_training(data_file,cnn_config,model_save,factor=factor)[0]\n",
    "            llm.append(model)\n",
    "            ds_model.append('CNN%1.1f'%factor)\n",
    "            perf.append(pr_perf)\n",
    "            c_t.append(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame({'LLM':llm,'Model':ds_model,'Performance':perf,'Cell Type':c_t})\n",
    "perf_df.to_csv('./results/LLM_CNN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
