{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "sys.path.append('../data_generation')\n",
    "import utils \n",
    "import numpy as np\n",
    "datalen = '5994'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"../data/CAGI/\"+datalen+\"/CAGI_onehot.h5\", \"r\")\n",
    "alt = file['alt']\n",
    "ref = file['ref']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NT zero-shot\n",
    "\n",
    "Cosine similarity between embeddings with different allele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2 model\n"
     ]
    }
   ],
   "source": [
    "import nucleotide_transformer\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "from tqdm import tqdm\n",
    "model_name = '500M_1000G'\n",
    "\n",
    "if '2B5' in model_name:\n",
    "    print('2B5_model')\n",
    "    embed_layer = 32\n",
    "elif 'huamn_ref' in model_name: \n",
    "    print('500M model')\n",
    "    embed_layer = 24\n",
    "elif '_v2' in model_name:\n",
    "    print('V2 model')\n",
    "    embed_layer = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = math.ceil(len(alt[0])/6)+1\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    embeddings_layers_to_save=(embed_layer,),\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=max_len,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [24:20<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# CLS = 3\n",
    "# PAD = 2\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "N, L, A = alt.shape\n",
    "batch_size = 50\n",
    "cagi_llr=[]\n",
    "for i in tqdm(range(0,N,batch_size)):\n",
    "    b_size = batch_size\n",
    "    if i + batch_size > N:\n",
    "        b_size = N-i\n",
    "    onehot = np.concatenate((ref[i:i+b_size],alt[i:i+b_size]))\n",
    "    seq = utils.onehot_to_seq(onehot)\n",
    "    token_out = tokenizer.batch_tokenize(seq)\n",
    "    token_id = [b[1] for b in token_out]\n",
    "    seq_pair = jnp.asarray(token_id,dtype=jnp.int32)\n",
    "    outs = forward_fn.apply(parameters, random_key, seq_pair)\n",
    "    for a in range(b_size):\n",
    "        ref_out = outs['embeddings_'+str(embed_layer)][a, 1:, :]\n",
    "        alt_out = outs['embeddings_'+str(embed_layer)][a+b_size, 1:, :]\n",
    "        cagi_llr.append((ref_out * alt_out).sum()/(jnp.linalg.norm(ref_out)*jnp.linalg.norm(alt_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = h5py.File('../data/CAGI/NT_zeroshot/'+'cagi_'+datalen+'_'+model_name+'.h5', 'w')\n",
    "output.create_dataset('llr', data=np.array(cagi_llr))\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats as stats\n",
    "input_length = '5994'\n",
    "model_name = '2B5_1000G'\n",
    "cagi_df = pd.read_csv('../data/CAGI/'+input_length+'/final_cagi_metadata.csv',\n",
    "                      index_col=0).reset_index()\n",
    "exp_list = cagi_df['8'].unique()\n",
    "target = cagi_df['6'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagi_result = h5py.File('/home/ztang/LLM_eval/data/CAGI/NT_zeroshot'+'/cagi_'+input_length+'_'+model_name+'.h5', 'r')\n",
    "cagi_llr = cagi_result['llr'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDLR\n",
      "-0.16390417169264007\n",
      "SORT1\n",
      "-0.10878043469983589\n",
      "F9\n",
      "-0.10292111437360503\n",
      "PKLR\n",
      "-0.0068942530809115435\n"
     ]
    }
   ],
   "source": [
    "perf = []\n",
    "for exp in ['LDLR','SORT1','F9','PKLR']:\n",
    "    sub_df = cagi_df[cagi_df['8'] == exp]\n",
    "    exp_target = np.array(target)[sub_df.index.to_list()]\n",
    "    exp_pred = np.squeeze(cagi_llr)[sub_df.index.to_list()]\n",
    "    exp_target = np.absolute(exp_target)\n",
    "    exp_pred = exp_pred\n",
    "    print(exp)\n",
    "    perf.append(stats.pearsonr(exp_pred,exp_target)[0])\n",
    "    print(stats.pearsonr(exp_pred,exp_target)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.164 -0.109 -0.102)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAGI embedding generation for lenti model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2B5_model\n"
     ]
    }
   ],
   "source": [
    "import nucleotide_transformer\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "from tqdm import tqdm\n",
    "model_name = '2B5_1000G'\n",
    "datalen='230'\n",
    "\n",
    "file = h5py.File(\"../data/CAGI/\"+datalen+\"/CAGI_onehot.h5\", \"r\")\n",
    "alt = file['alt']\n",
    "ref = file['ref']\n",
    "\n",
    "if '2B5' in model_name:\n",
    "    print('2B5_model')\n",
    "    embed_layer = 32\n",
    "\n",
    "max_len = len(alt[0])//6+len(alt[0])%6+1\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    embeddings_layers_to_save=(embed_layer,),\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=max_len,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [27:14<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# CLS = 3\n",
    "# PAD = 2\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "N, L, A = alt.shape\n",
    "batch_size = 50\n",
    "ref_out = []\n",
    "alt_out = []\n",
    "for i in tqdm(range(0,N,batch_size)):\n",
    "    ref_seq = ref[i:i+batch_size]\n",
    "    alt_seq = alt[i:i+batch_size]\n",
    "    ref_seq = utils.onehot_to_seq(ref_seq)\n",
    "    alt_seq = utils.onehot_to_seq(alt_seq)\n",
    "\n",
    "    ref_token = tokenizer.batch_tokenize(ref_seq)\n",
    "    alt_token = tokenizer.batch_tokenize(alt_seq)\n",
    "    ref_token = [b[1] for b in ref_token]\n",
    "    alt_token = [b[1] for b in alt_token]\n",
    "\n",
    "    ref_pair = jnp.asarray(ref_token,dtype=jnp.int32)\n",
    "    alt_pair = jnp.asarray(alt_token,dtype=jnp.int32)\n",
    "\n",
    "    ref_output = forward_fn.apply(parameters, random_key, ref_pair)['embeddings_'+str(embed_layer)]\n",
    "    alt_output = forward_fn.apply(parameters, random_key, alt_pair)['embeddings_'+str(embed_layer)]\n",
    "\n",
    "    ref_out.extend(np.asarray(ref_output))\n",
    "    alt_out.extend(np.asarray(alt_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = h5py.File('../data/CAGI/230_embed/'+'NT.h5', 'w')\n",
    "output.create_dataset('ref', data=np.array(ref_out))\n",
    "output.create_dataset('alt', data=np.array(alt_out))\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
