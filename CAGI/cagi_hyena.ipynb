{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztang/.conda/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../data_generation/')\n",
    "import utils\n",
    "import subprocess\n",
    "import torch\n",
    "import glob\n",
    "from transformers import PreTrainedModel\n",
    "import re\n",
    "from standalone_hyenadna import HyenaDNAModel\n",
    "from standalone_hyenadna import CharacterTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import h5py \n",
    "import scipy.stats\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "# helper 2\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\"\"\"\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = 'hyenadna-tiny-1k-d256'\n",
    "max_length = 1_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    '../data_generation/checkpoints',\n",
    "    pretrained_model_name,\n",
    ").to('cuda')\n",
    "model.eval()\n",
    "\n",
    "# create tokenizer, no training involved :)\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters\n",
    "    model_max_length=max_length,\n",
    ")\n",
    "datalen = str(1000-2)\n",
    "file = h5py.File(\"../data/CAGI/\"+datalen+\"/CAGI_onehot.h5\", \"r\")\n",
    "alt = file['alt']\n",
    "ref = file['ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [01:02<00:00,  5.94it/s]\n"
     ]
    }
   ],
   "source": [
    "N, L, A = alt.shape\n",
    "batch_size = 50\n",
    "cos = []\n",
    "dot = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "for i in tqdm(range(0,N,batch_size)):\n",
    "    b_size = batch_size\n",
    "    if i + batch_size > N:\n",
    "        b_size = N-i\n",
    "    onehot = np.concatenate((ref[i:i+b_size],alt[i:i+b_size]))\n",
    "    seq = utils.onehot_to_seq(onehot)\n",
    "    input_ids = tokenizer(list(seq), return_tensors=\"pt\",)[\"input_ids\"].to('cuda')\n",
    "    with torch.inference_mode():\n",
    "        hidden_states = model(input_ids).cpu().detach().numpy()\n",
    "    for a in range(b_size):\n",
    "        ref_out = hidden_states[a]\n",
    "        alt_out = hidden_states[a+b_size]\n",
    "        cos.append((ref_out * alt_out).sum()/(np.linalg.norm(ref_out)*np.linalg.norm(alt_out)))\n",
    "        dot.append((ref_out * alt_out).sum())\n",
    "        l1.append(np.absolute(ref_out - alt_out).sum())\n",
    "        l2.append(np.square(ref_out - alt_out).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ztang/LLM_eval/CAGI/cagi_hyena.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgalaxy1/home/ztang/LLM_eval/CAGI/cagi_hyena.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39;49m\u001b[39m../data/CAGI/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcagi_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mdatalen\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhyena.h5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgalaxy1/home/ztang/LLM_eval/CAGI/cagi_hyena.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m output\u001b[39m.\u001b[39mcreate_dataset(\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m, data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(cos))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgalaxy1/home/ztang/LLM_eval/CAGI/cagi_hyena.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m output\u001b[39m.\u001b[39mcreate_dataset(\u001b[39m'\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m'\u001b[39m, data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(dot))\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "output = h5py.File('../data/CAGI/'+'cagi_'+datalen+'_'+'hyena.h5', 'w')\n",
    "output.create_dataset('cosine', data=np.array(cos))\n",
    "output.create_dataset('dot', data=np.array(dot))\n",
    "output.create_dataset('l1', data=np.array(l1))\n",
    "output.create_dataset('l2', data=np.array(l2))\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats as stats\n",
    "datalen = '998'\n",
    "cagi_df = pd.read_csv('../data/CAGI/'+datalen+'/final_cagi_metadata.csv',\n",
    "                      index_col=0).reset_index()\n",
    "target = cagi_df['6'].values.tolist()\n",
    "exp_list = cagi_df['8'].unique()\n",
    "cagi_result = h5py.File('../data/CAGI/'+'cagi_'+datalen+'_'+'hyena.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine\n",
      "LDLR\n",
      "-0.18198321567003217\n",
      "SORT1\n",
      "0.02906028358401116\n",
      "F9\n",
      "-0.039868096952063246\n",
      "PKLR\n",
      "0.02110880773094938\n",
      "dot\n",
      "LDLR\n",
      "-0.10038456364102219\n",
      "SORT1\n",
      "0.2792669648722809\n",
      "F9\n",
      "-0.0036651786753944416\n",
      "PKLR\n",
      "-0.08001239470975105\n",
      "l1\n",
      "LDLR\n",
      "0.033906396737050384\n",
      "SORT1\n",
      "0.00802440182088367\n",
      "F9\n",
      "0.07154549609659909\n",
      "PKLR\n",
      "9.99546991168468e-05\n",
      "l2\n",
      "LDLR\n",
      "0.1831921825264321\n",
      "SORT1\n",
      "-0.028138878273324765\n",
      "F9\n",
      "0.038730829156372895\n",
      "PKLR\n",
      "-0.018634354570632585\n"
     ]
    }
   ],
   "source": [
    "perf = []\n",
    "for key in cagi_result.keys():\n",
    "    print(key)\n",
    "    cagi_llr = cagi_result[key]\n",
    "    for exp in ['LDLR','SORT1','F9','PKLR']:\n",
    "        sub_df = cagi_df[cagi_df['8'] == exp]\n",
    "        exp_target = np.array(target)[sub_df.index.to_list()]\n",
    "        exp_pred = np.squeeze(cagi_llr)[sub_df.index.to_list()]\n",
    "        exp_target = np.absolute(exp_target)\n",
    "        exp_pred = exp_pred\n",
    "        print(exp)\n",
    "        perf.append(stats.pearsonr(exp_pred,exp_target)[0])\n",
    "        print(stats.pearsonr(exp_pred,exp_target)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
